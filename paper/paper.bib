%-----------------------------------------------------------------------------------------------------------------------
% code repository
%-----------------------------------------------------------------------------------------------------------------------

@misc{caption-captions-2023,
  author = {Christoper Dugan, Mouhcine Aitounejjar, Walker Stevens, Xuan Xu},
  title = {Image Captioning with Deep Learning},
  year = {2023},
  publisher = {GaTech GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.gatech.edu/maitounejjar3/cs7643-project}}
}

@misc{kim2016sequencelevel,
      title={Sequence-Level Knowledge Distillation},
      author={Yoon Kim and Alexander M. Rush},
      year={2016},
      eprint={1606.07947},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

%-----------------------------------------------------------------------------------------------------------------------
% datasets
%-----------------------------------------------------------------------------------------------------------------------

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}



%-----------------------------------------------------------------------------------------------------------------------

@article{flicker8kdataset,
title= {Flickr8k Dataset},
keywords= {},
author= {Micah Hodosh and Peter Young and Julia Hockenmaier},
abstract= {8,000 photos and up to 5 captions for each photo.

	We introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. â€¦ The images were chosen from six different Flickr groups, and tend not to contain any well-known people or locations, but were manually selected to depict a variety of scenes and situations

https://i.imgur.com/6RxAndT.png

## Citation

Hodosh, Micah, Peter Young, and Julia Hockenmaier. "Framing image description as a ranking task: Data, models and evaluation metrics." Journal of Artificial Intelligence Research 47 (2013): 853-899.

},
terms= {},
license= {},
superseded= {},
url= {}
}

%-----------------------------------------------------------------------------------------------------------------------
% papers
%-----------------------------------------------------------------------------------------------------------------------

@inproceedings{zeiler2014visualizing,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13},
  pages={818--833},
  year={2014},
  organization={Springer}
}

@article{simonyan2013deep,
  title={Deep inside convolutional networks: Visualising image classification models and saliency maps},
  author={Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1312.6034},
  year={2013}
}


@article{chen2015COCOCaptions,
  title={Microsoft COCO Captions: Data Collection and
Evaluation Server},
  author={Chen, Xinlei and Lin, Tsung-Yi and et. al},
  journal={arXiv preprint arXiv:1504.00325v2},
  year={2015}
}

@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}

@inproceedings{xu2015show,
  title={Show, attend and tell: Neural image caption generation with visual attention},
  author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={2048--2057},
  year={2015},
  organization={PMLR}
}


@misc{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network},
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{li2021transferring,
      title={Transferring Knowledge with Attention Distillation for Multi-Domain Image-to-Image Translation},
      author={Runze Li and Tomaso Fontanini and Luca Donati and Andrea Prati and Bir Bhanu},
      year={2021},
      eprint={2108.07466},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{cho2019efficacy,
      title={On the Efficacy of Knowledge Distillation},
      author={Jang Hyun Cho and Bharath Hariharan},
      year={2019},
      eprint={1910.01348},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{furlanello2018born,
      title={Born Again Neural Networks},
      author={Tommaso Furlanello and Zachary C. Lipton and Michael Tschannen and Laurent Itti and Anima Anandkumar},
      year={2018},
      eprint={1805.04770},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{zagoruyko2017paying,
      title={Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer},
      author={Sergey Zagoruyko and Nikos Komodakis},
      year={2017},
      eprint={1612.03928},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{heo2019comprehensive,
      title={A Comprehensive Overhaul of Feature Distillation},
      author={Byeongho Heo and Jeesoo Kim and Sangdoo Yun and Hyojin Park and Nojun Kwak and Jin Young Choi},
      year={2019},
      eprint={1904.01866},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{park2019relational,
      title={Relational Knowledge Distillation},
      author={Wonpyo Park and Dongju Kim and Yan Lu and Minsu Cho},
      year={2019},
      eprint={1904.05068},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{sagar2018,
  author = {Sagar Vinodababu},
  title = {A PyTorch Tutorial to Image Captioning},
  year = {2018},
  publisher = {GaTech GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning}}
}

@misc{lavis2022,
      title={LAVIS: A Library for Language-Vision Intelligence}, 
      author={Dongxu Li and Junnan Li and Hung Le and Guangsen Wang and Silvio Savarese and Steven C. H. Hoi},
      year={2022},
      eprint={2209.09019},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{cornia2022explaining,
  title={Explaining transformer-based image captioning models: An empirical analysis},
  author={Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
  journal={AI Communications},
  volume={35},
  number={2},
  pages={111--129},
  year={2022},
  publisher={IOS Press}
}

%-----------------------------------------------------------------------------------------------------------------------

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

%-----------------------------------------------------------------------------------------------------------------------

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}

}

%-----------------------------------------------------------------------------------------------------------------------

@inproceedings{parmar2018image,
  title={Image transformer},
  author={Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  booktitle={International conference on machine learning},
  pages={4055--4064},
  year={2018},
  organization={PMLR}
}

%-----------------------------------------------------------------------------------------------------------------------

@article{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

%-----------------------------------------------------------------------------------------------------------------------

@article{huang2020pixel,
  title={Pixel-bert: Aligning image pixels with text by deep multi-modal transformers},
  author={Huang, Zhicheng and Zeng, Zhaoyang and Liu, Bei and Fu, Dongmei and Fu, Jianlong},
  journal={arXiv preprint arXiv:2004.00849},
  year={2020}
}

%-----------------------------------------------------------------------------------------------------------------------

@article{wang2020vd,
  title={Vd-bert: A unified vision and dialog transformer with bert},
  author={Wang, Yue and Joty, Shafiq and Lyu, Michael R and King, Irwin and Xiong, Caiming and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2004.13278},
  year={2020}
}

%-----------------------------------------------------------------------------------------------------------------------

@inproceedings{huang-etal-2019-unicoder,
    title = "{U}nicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks",
    author = "Huang, Haoyang  and
      Liang, Yaobo  and
      Duan, Nan  and
      Gong, Ming  and
      Shou, Linjun  and
      Jiang, Daxin  and
      Zhou, Ming",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1252",
    doi = "10.18653/v1/D19-1252",
    pages = "2485--2494",
    abstract = "We present Unicoder, a universal language encoder that is insensitive to different languages. Given an arbitrary NLP task, a model can be trained with Unicoder using training data in one language and directly applied to inputs of the same task in other languages. Comparing to similar efforts such as Multilingual BERT and XLM , three new cross-lingual pre-training tasks are proposed, including cross-lingual word recovery, cross-lingual paraphrase classification and cross-lingual masked language model. These tasks help Unicoder learn the mappings among different languages from more perspectives. We also find that doing fine-tuning on multiple languages together can bring further improvement. Experiments are performed on two tasks: cross-lingual natural language inference (XNLI) and cross-lingual question answering (XQA), where XLM is our baseline. On XNLI, 1.8{\%} averaged accuracy improvement (on 15 languages) is obtained. On XQA, which is a new cross-lingual dataset built by us, 5.5{\%} averaged accuracy improvement (on French and German) is obtained.",
}

%-----------------------------------------------------------------------------------------------------------------------

@article{su2019vl,
  title={Vl-bert: Pre-training of generic visual-linguistic representations},
  author={Su, Weijie and Zhu, Xizhou and Cao, Yue and Li, Bin and Lu, Lewei and Wei, Furu and Dai, Jifeng},
  journal={arXiv preprint arXiv:1908.08530},
  year={2019}
}

%-----------------------------------------------------------------------------------------------------------------------

@article{qi2020imagebert,
  title={Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data},
  author={Qi, Di and Su, Lin and Song, Jia and Cui, Edward and Bharti, Taroon and Sacheti, Arun},
  journal={arXiv preprint arXiv:2001.07966},
  year={2020}
}

%-----------------------------------------------------------------------------------------------------------------------

@inproceedings{gao2020fashionbert,
  title={Fashionbert: Text and image matching with adaptive loss for cross-modal retrieval},
  author={Gao, Dehong and Jin, Linbo and Chen, Ben and Qiu, Minghui and Li, Peng and Wei, Yi and Hu, Yi and Wang, Hao},
  booktitle={Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2251--2260},
  year={2020}
}

%-----------------------------------------------------------------------------------------------------------------------

@article{li2019visualbert,
  title={Visualbert: A simple and performant baseline for vision and language},
  author={Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:1908.03557},
  year={2019}
}

%-----------------------------------------------------------------------------------------------------------------------

@inproceedings{chen2020uniter,
  title={Uniter: Universal image-text representation learning},
  author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXX},
  pages={104--120},
  year={2020},
  organization={Springer}
}

%-----------------------------------------------------------------------------------------------------------------------

@article{tan2019lxmert,
  title={Lxmert: Learning cross-modality encoder representations from transformers},
  author={Tan, Hao and Bansal, Mohit},
  journal={arXiv preprint arXiv:1908.07490},
  year={2019}
}

%-----------------------------------------------------------------------------------------------------------------------

@article{li2022mplug,
  title={mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections},
  author={Li, Chenliang and Xu, Haiyang and Tian, Junfeng and Wang, Wei and Yan, Ming and Bi, Bin and Ye, Jiabo and Chen, Hehong and Xu, Guohai and Cao, Zheng and others},
  journal={arXiv preprint arXiv:2205.12005},
  year={2022}
}

%-----------------------------------------------------------------------------------------------------------------------
@article{blip2022,
  author       = {Junnan Li and
                  Dongxu Li and
                  Caiming Xiong and
                  Steven C. H. Hoi},
  title        = {{BLIP:} Bootstrapping Language-Image Pre-training for Unified Vision-Language
                  Understanding and Generation},
  journal      = {CoRR},
  volume       = {abs/2201.12086},
  year         = {2022},
  url          = {https://arxiv.org/abs/2201.12086},
  eprinttype    = {arXiv},
  eprint       = {2201.12086},
  timestamp    = {Wed, 02 Feb 2022 15:00:01 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2201-12086.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
%-----------------------------------------------------------------------------------------------------------------------
